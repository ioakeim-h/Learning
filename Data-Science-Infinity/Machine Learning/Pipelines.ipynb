{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14300bfb",
   "metadata": {},
   "source": [
    "# Pipelines - Basic Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db409256",
   "metadata": {},
   "source": [
    "Before building a machine learning model we must first go through important procedures such as data cleaning (e.g. dealing with missing values), data preparation (e.g. encoding categorical variables) and even the specific functionality for instantiating and fitting the model.\n",
    "\n",
    "A pipeline object is a sequence of instructions that tie such processes together, automating machine learning workflows whenever new data is available. For example, it can be saved in a web application and any time new data comes in to be predicted or classified by our model, all of the appropriate preprocessing steps will be applied to the data and a prediction would be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd289f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d597a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sample data\n",
    "\n",
    "df = pd.read_csv(\"Desktop\\Learning\\DATA SCIENCE INFINITY\\Machine Learning\\Model Building\\data\\pipeline_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72498ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into input and output objects\n",
    "\n",
    "X = df.drop([\"purchase\"], axis = 1)\n",
    "y = df[\"purchase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c3e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07377089",
   "metadata": {},
   "source": [
    "Numeric and categorical columns are handled differently. In this example, numeric columns will be scaled and their missing values will be imputed using the mean. Scaling and mean imputation are not relevant to categorical columns where missing values will be replaced with a constant and one-hot-encoding will be used. \n",
    "\n",
    "The procedures for each column type will need to be stored in their own object before bringing them together in one large pipeline. These objects are known as transformers and they are basically mini pipelines waiting to be combined.\n",
    "\n",
    "Let's start by storing each of the numeric variables into a list and then do the same for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5abc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify numeric and categorical features\n",
    "\n",
    "numeric_features = [\"age\", \"credit_score\"]\n",
    "categorical_features = [\"gender\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da2851",
   "metadata": {},
   "source": [
    "## Set up Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ec168",
   "metadata": {},
   "source": [
    "Using the objects created above, we can *set up a transformer for each type of column*. This is done using the Pipeline() function from sklearn. Inside the parenthesis of this function we specify the preprocessing steps we want to apply and we do this using the *steps* parameter. This parameter takes a list of tuples including the **what's** and the **how's**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3a580",
   "metadata": {},
   "source": [
    "For our numeric columns, we first want to impute any missing values using the the SimpleImputer(). Then, we want to scale the data using the StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f1b83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Feature Transformer\n",
    "\n",
    "numeric_transformer = Pipeline(steps = [(\"imputer\", SimpleImputer()), # Here \"imputer\" is used to instantiate SimpleImputer()\n",
    "                                        (\"scaler\", StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74dec9",
   "metadata": {},
   "source": [
    "The procedure for categorical columns differs slightly because we want to replace missing values with a constant instead of using the mean like we did for numeric columns. We will also one-hot-encode the data instead of scale it. The *handle_unknown* parameter of the OneHotEncoder() is set to ignore which means that any new categories that may come from new data will be ignored by transforming the entire column to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "191675db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Feature Transformer\n",
    "\n",
    "categorical_transformer = Pipeline(steps = [(\"imputer\", SimpleImputer(strategy = \"constant\", fill_value = \"U\")),\n",
    "                                        (\"ohe\", OneHotEncoder(handle_unknown = \"ignore\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0fe1",
   "metadata": {},
   "source": [
    "Now that a transformer object has been specified for each column type, we can pass both of them into one overall pipeline using the ColumnTransformer(). The procedure is similar to the above transformers, but we also need to provide the columns that we want to apply the logic to. We have already stored the different column types into a couple of lists, so we will be using those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2feb264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer(transformers = [(\"numeric\" , numeric_transformer, numeric_features),\n",
    "                                                          (\"categorical\", categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d46085",
   "metadata": {},
   "source": [
    "## Apply the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ec69f",
   "metadata": {},
   "source": [
    "To apply the preprocessing pipeline, we are going to create another larger pipeline object that is going to specify that we want to receive data, pass it through the preprocessing pipeline, and then pass that onto a specific model. We will do this for a logistic regression model and a random forest model. In doing so, we are going to see how a pipeline can help us compare the performance of models on exactly the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d86125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "clf = Pipeline(steps = [(\"preprocessing_pipeline\", preprocessing_pipeline),\n",
    "                       (\"classifier\", LogisticRegression(random_state = 42))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526a51b",
   "metadata": {},
   "source": [
    "Our classifier pipeline is now ready to go. Let's train the model and see exactly what is going to happen when the data gets passed through that pipeline. We can then carry out the predictions on the test set and get an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe8fa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing_pipeline',\n",
       "                 ColumnTransformer(transformers=[('numeric',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['age', 'credit_score']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='U',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['gender'])])),\n",
       "                ('classifier', LogisticRegression(random_state=42))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b059e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb8d05",
   "metadata": {},
   "source": [
    "We can see that our model has received the test data which is unprocessed. If we take a look at our X_test object we will see that it is in its original state. That data has been passed through the preprocessing pipeline where numerical columns and categorical columns were dealt with accordingly, and finally passed into the trained classifier model allowing us to get predictions.\n",
    "\n",
    "Let's repeat this for the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0717c586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "clf = Pipeline(steps = [(\"preprocessing_pipeline\", preprocessing_pipeline),\n",
    "                       (\"classifier\", RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_class = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92213fd2",
   "metadata": {},
   "source": [
    "Coincidentally, on our sample data both models got the same accuracy score, but we were indeed applying separate types of models to the data. Moreover, since we are passing in unprocessed data, we could pass in any new data as well! This brings us to the next part where we will be passing in some brand new data to our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27aa8d8",
   "metadata": {},
   "source": [
    "## Save the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513fbc36",
   "metadata": {},
   "source": [
    "Without pipelines, in order to process any new data in the same way that the training data had been processed, we potentially have to save several objects including the model itself, an imputer object, an encoding object etc. By using pipelines, however, we have one clean object making it very easy to save our pipeline into another location, even somewhere like a web application. Any new data coming in from a user, that needs to be classified or predicted by our model, all of the appropriate preprocessing steps would be applied to the data and the prediction would be made. \n",
    "\n",
    "Let's go through a simple illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "128ceba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desktop\\\\Learning\\\\DATA SCIENCE INFINITY\\\\Machine Learning\\\\Model Building\\\\data\\\\model.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, \"Desktop\\Learning\\DATA SCIENCE INFINITY\\Machine Learning\\Model Building\\data\\model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ac805",
   "metadata": {},
   "source": [
    "### Import pipeline object and predict on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025881d1",
   "metadata": {},
   "source": [
    "For demonstration purposes, the kernel was restarted and the relevant libraries were loaded before importing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd13bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a38a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline\n",
    "\n",
    "clf = joblib.load(\"Desktop\\Learning\\DATA SCIENCE INFINITY\\Machine Learning\\Model Building\\data\\model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc8c3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>credit_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age gender  credit_score\n",
       "0  25.0      M           200\n",
       "1   NaN      F           100\n",
       "2  50.0    NaN           500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new data\n",
    "\n",
    "new_data = pd.DataFrame({\"age\" : [25,np.nan,50],\n",
    "                        \"gender\" : [\"M\", \"F\", np.nan],\n",
    "                        \"credit_score\" : [200,100,500]})\n",
    "\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c19c0",
   "metadata": {},
   "source": [
    "Let's see what we need to do to get some predictions on the new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40bfc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass new data in and receive predictions\n",
    "\n",
    "clf.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c78d727",
   "metadata": {},
   "source": [
    "That's it! The model has received the input data, passed it through the preprocessing pipeline, then through the trained Random Forest classifier model, and then provided us with the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7707c",
   "metadata": {},
   "source": [
    "Pipelines are super clean and slick but they do take away the ease of a \"step-by-step\" understanding which is key at the start of the process. It may be best to work it all out manually when preprocessing and building the model so we can keep a closer eye on what the data looks like at each step. When done, we can tie it all together into a nice, neat pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
