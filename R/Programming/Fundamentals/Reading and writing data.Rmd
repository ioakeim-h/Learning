---
title: "Reading and Writing Data, Data formats, and Connections"
author: "Akis"
date: "1/7/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<center> <h1> Reading data into R </h1> </center>

There are a few principal functions for reading data into R.

* `read.table()`, `read.csv()`, for reading tabular data (dataframe)
* `readLines`, for reading lines of a text file (returns a character vector)
* `source `, for reading R code files (inverse of dump)
* `dget`, for reading R code files (inverse of dput)
* `load`, for reading in saved workspaces
* `unserialize`, for reading single R objects in binary form

<center> <h1> Writing Data </h1> </center>

There are analogous functions for writing data to files.

* `write.table()`
* `writeLines()`
* `dump`
* `dput`
* `save`
* `serialize`

# Reading Data Files with read.table

the `read.table()` function is one of the most common functions for reading data. It has a few important arguments.

* `file`, the name of a file, or a connection
* `header`, logical (TRUE/FALSE) indicating if the file has a header line (variable names)
* `sep`, a string indicating how the columns are separated
* `colClasses`, a character vector indicating the class of each column in the dataset
* `nrows`, the number of rows in the dataset
* `comment.char`, a character string indicating the comment character
* `skip`, the  number of lines to skip from the beginning
* `stringsAsFactors`, should character variables be coded as factors?

For small to moderately sized datasets, you can usually call `read.table()` without specifying any other arguments.

```{r}
# data <- read.table("foo.txt")
```

# Reading in Larger Datasets with read.table

With large data sets we need to consider the following things:

* Read the help page for `read.table` which contains many hints
* Make a rough calculation of the memory required to store your data set. If it is larger that the amount of RAM on your computer, you can probably stop right there. [Check out this guide.](https://medium.com/@robertopreste/tabular-data-memory-requirements-9881d2bf747a)
* Set `comment.char = ` (blank) if there are no commented lines in your file.
\
\
* Use the `colClasses` argument. Specifying this option instead of using the default can make `read.table()` run MUCH faster. You have to know the class of each column in your data frame. If all of the columns are "numeric", for example, then you can just set `colClasses = "numeric"`. A quick way to figure out the classes of each column is to read only the first 100 or 1000 rows of your data set by specifying the `nrows` argument and then looping over each of the columns using `sapply()` and call the `class` function. Then, we can read the entire data set as shown below:

```{r}
# initial <- read.table("datatable.txt", nrows = 100)
# classes <- sapply(initial, classes)
# tabAll <- read.table("datatable.txt", colClasses = classes)
```

# Textual Data Formats

Beyond the tabular format, the CSV file, there are other textual formats that are different from tabular data. Textual formats are good for storing data. The two main functions for writing out data are *dumping* and *dputing*.

* `dump()` and `dput()` provide an edit-able textual format
* They preserve [*metadata*](https://www.google.com/search?q=what+are+metadata&rlz=1C5CHFA_enGB922GB922&oq=what+are+metadata&aqs=chrome..69i57j0i512l9.1795j0j4&sourceid=chrome&ie=UTF-8) so they won't have to be specified all-over again!
* If there is a corruption in the file it can be easier to fix the problem
* But they are not space-efficient

## Dput-ting R objects

Another way to pass data around is by deparsing the R object with `dput` and reading it back in with `dget`.

```{r}
y <- data.frame(a = 1, b = "a")
dput(y)
structure(list(a = 1,
               b = structure(1L, .Label = "a",
                             class = "factor")),
          .Names = c("a","b"), row.names = c(NA, -1L),
          class = "data.frame")
dput(y, file = "y.R")
new.y <- dget("y.R")
new.y
```

## Dumping R objects

Multiple objects can be deparsed using the dump function and read back in using `source`.

```{r}
x <- "foo"
y <- data.frame(a = 1, b = "a")
dump(c("x", "y"), file = "data.R")
rm(x,y)
source("data.R")
y
x
```


# Connections: Reading Lines of a Text File

Data are read in using *connection* interfaces. For example, when you call read.table and you pass the name of a file, what it does behind the scenes is open a file connection to that file and read from that file. [Connections](https://colinfay.me/r-data-import-export/connections.html) can be made to files...

* `file`, opens a connection to a file
* `gzfile`, opens a connection to a file compressed with gzip
* `bzfile`, opens a connection to a file compressed with bzip2
* `url`, opens a connection to a webpage

## File connections

```{r eval=FALSE}
str(file)
function(description = "", open = "", blocking = TRUE,
         encoding = getOption("encoding"))
```

* description is the name of the file
* open is a code indicating
* "r" read only
* "w" writing (and initializing a new file)
* "a" appending
* "rb", "wb", "ab" reading, writing, or appending in binary mode "(Windows)

We do not often have to deal with the connection interface directly but in some cases we do. For example if we want to read parts of a file.

### Reading lines of a text file

```{r eval=FALSE}
con <- gzfile("words.gz")
x <- readLines(con, 10)
x

# writeLines takes a character vector and writes each element one line at a time to a text file.
```


