{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ETL: Extract</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ETL, extraction involves retrieving data from persistent storage into memory. This involves identifying the source systems, determining the data to be extracted, and accessing the source data. The data engineer may need to use various techniques to extract data, such as connecting to a database, accessing a file system, or consuming data from an API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting with Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Files\n",
    "\n",
    "Data can be extracted from unstructured (e.g. book) or structured text files (e.g. csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/path/to/data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Files\n",
    "\n",
    "Many web services use JSON files to communicate data which hold information in a semi-structured way. <br>\n",
    "JSON files can be parsed into Python and converted to a Pandas DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file into a Pandas DataFrame\n",
    "df = pd.read_json('path/to/json/file')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If your JSON file has nested objects, you may need to use the pd.json_normalize() method with appropriate arguments to flatten the nested objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file into a Python object\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the Python object into a pandas DataFrame\n",
    "df = pd.json_normalize(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n",
    "\n",
    "The internet allows communication between clients (like web browsers or applications) and servers (like computers) using the HTTP protocol. Clients send requests to servers using URLs that specify the type of request (e.g. GET, POST, PUT, DELETE) and any necessary data or parameters. The server processes the request and sends back a response containing data or other content.\n",
    "\n",
    "APIs (Application Programming Interfaces) are a type of server that provides data in a standardized format (usually JSON or XML) that software applications can use to communicate with each other.\n",
    "\n",
    "Python's Requests library simplifies the process of making HTTP requests from a Python program. It allows you to create a request object with the necessary information and send it to the server. Once the server responds, you can access the response data and other information using methods and properties of the response object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the API endpoint you want to access\n",
    "response = requests.get(\"https://api.example.com/data\")\n",
    "\n",
    "# Parse response data as a JSON object\n",
    "data = response.json()\n",
    "\n",
    "# Convert the JSON data to a Pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases\n",
    "\n",
    "Extracting data from databases is probably the most common. For this to happen, a connection string (or URI) is needed which holds information on how to connect to the database. <br>\n",
    "It typically contains the database type (e.g. PostgreSQL), username and password, host and port. This connection can be used to create a database engine which can be passed to packages like Pandas to interact with the data in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "# Create connection\n",
    "connection_uri = \"postgresql://user:password@host:port\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Pandas .read_sql() method\n",
    "df = pd.read_sql(\"SELECT * FROM my_table\", db_engine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Batch Processing\n",
    "\n",
    "Batch processing is a way of processing data in batches or chunks, rather than all at once. It is useful when working with large datasets that may not fit into memory. <br>\n",
    "Pandas supports batch processing through its powerful dataframe and series objects. It allows us to read and write data into chunks, but if the code is ran multiple times, duplicate data will be generated in the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file in batches of 1000 rows\n",
    "for chunk in pd.read_csv('input_file.csv', chunksize=1000):\n",
    "    # Do some processing on the chunk\n",
    "    processed_chunk = chunk.apply(lambda x: x + 1)\n",
    "\n",
    "    # Export the processed chunk to a CSV file\n",
    "    processed_chunk.to_csv('output_file.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting with PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, you can extract data from a variety of sources using the read method of the SparkSession object. A SparkSession can be created like this: <br>\n",
    "\n",
    "```SparkSession.builder.appName(\"name_of_app\").getOrCreate()``` \n",
    "\n",
    "1. SparkSession.builder: This creates a Builder object that is used to configure and create a SparkSession.\n",
    "\n",
    "2. appName(\"name_of_app\"): This sets the name of the application, which is used to identify the Spark application in the Spark UI and logs.\n",
    "\n",
    "3. getOrCreate(): This method returns a SparkSession or creates a new one if it does not already exist in the current context. This method is useful when you want to reuse an existing SparkSession across multiple code files or when running code in a notebook environment.\n",
    "\n",
    "A SparkSession can be used to interact with Spark and perform distributed data processing. <br>\n",
    "Once the SparkSession is created, you can use it to read and write data, create DataFrames, perform transformations, and run Spark jobs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"path/to/csv/file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Pandas' ```.read_json()``` method, PySpark's DataFrameReader is capable of handling nested JSON structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
    "\n",
    "# Read a JSON file into a DataFrame\n",
    "df = spark.read.json(\"path/to/json/file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n",
    "\n",
    "To extract data from an API using PySpark, you can use the requests library to make HTTP requests to the API, and then parse the response data using PySpark's DataFrame API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
    "\n",
    "# Send a GET request to the API endpoint\n",
    "response = requests.get(\"https://api.example.com/data\")\n",
    "\n",
    "# Convert the response data to a PySpark DataFrame\n",
    "df = spark.createDataFrame([response.json()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases\n",
    "\n",
    "To extract data from a SQL database using PySpark, you can use the JDBC connector. In the example below,\n",
    "the JDBC connector is used to connect to a PostgreSQL database running on the local machine, using the url, table, and properties variables to specify the database connection properties. The spark.read.jdbc method is then used to read data from the database into a PySpark DataFrame, which can be used to analyze and manipulate the data. You can then display the DataFrame schema and the first few rows of data to verify that the data was loaded correctly, using the df.printSchema() and df.show() methods, respectively.\n",
    "\n",
    "Note that the specific method for connecting to a SQL database will depend on the database you're using and the specific data you want to extract. You may need to adjust the code to handle authentication, database drivers, or other database-specific features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
    "\n",
    "# Define the JDBC connection properties\n",
    "url = \"jdbc:postgresql://localhost:5432/my_database\"\n",
    "table = \"my_table\"\n",
    "properties = {\n",
    "    \"user\": \"my_user\",\n",
    "    \"password\": \"my_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Read data from the database into a DataFrame\n",
    "df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "\n",
    "# Display the DataFrame schema\n",
    "df.printSchema()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
