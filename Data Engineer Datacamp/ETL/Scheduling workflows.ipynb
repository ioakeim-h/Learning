{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Apache Airflow</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow is an open-source platform for creating, scheduling, and monitoring workflows or pipelines, which can be used to manage various types of data processing tasks such as ETL jobs, machine learning pipelines, and data analysis workflows. It provides a flexible and extensible framework for building and managing complex workflows, allowing users to define tasks, dependencies, and scheduling requirements in Python code.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Operators: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to have a function that describes the full ETL behaviour. This function can then be used to extract, transform, and load the data altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_engines = {...} # Needs to be configured\n",
    "\n",
    "def etl():\n",
    "    # Extract function\n",
    "    df = extract_table_to_df(\"table\", db_engines[\"engine_A\"])\n",
    "    \n",
    "    # Transform function\n",
    "    transformed_df = transform(df)\n",
    "    \n",
    "    # Load function\n",
    "    load(transformed_df)\n",
    "\n",
    "\n",
    "def extract_table_to_df(table, db_engine):\n",
    "    ...\n",
    "\n",
    "def transform(df):\n",
    "    ...\n",
    "\n",
    "def load(transformed_df):\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling Workflows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Airflow, DAG (Directed Acyclic Graph) is a way to define and orchestrate a workflow. A DAG is a collection of tasks, where each task represents a specific step in the workflow. The tasks are connected to each other in a specific order, and the connections between them define the flow of the data or the logic in the workflow. A DAG is defined using Python code, and it consists of a set of tasks, their dependencies, and their configuration options.\n",
    "\n",
    "<br>**<u>Tasks in a DAG</u>**\n",
    "\n",
    "Each task is a discrete piece of code or a command that needs to be run, such as executing a SQL query, running a Python script, or copying a file from one location to another. Each task in Airflow is implemented as an operator, which is a Python class that defines the logic for the task. For example, if you want to create a task that runs a Python script, you can use the built-in `PythonOperator` to execute the script. Other built-in operators include the `BashOperator` and the `S3CopyObjectOperator`, but customer operators that can handle more specific tasks can also be created.\n",
    "\n",
    "*Note:* Although tasks are implemented as operators, they are different concepts. While an operator is a class that defines the logic for a task, a task is a particular instance of that class within a DAG. For example, you might define a DAG with three tasks:\n",
    "\n",
    "- `ExtractData` task, which uses the `PythonOperator` to extract data from a source system\n",
    "- `TransformData` task, which uses the `BashOperator` to run a shell script to transform the data\n",
    "- `LoadData` task, which uses the `MySQLOperator` to load the transformed data into a MySQL database.\n",
    "\n",
    "<br>**<u>Task dependencies</u>**\n",
    "\n",
    "Dependencies refer to the relationships between tasks in a DAG. They determine the order in which tasks should be executed, ensuring that the output of one task is available before the next task can start. A task can have one or more dependencies, which can be other tasks in the same DAG or external dependencies such as data availability, file existence, or database state. There are two types of dependencies in Airflow: upstream dependencies and downstream dependencies.\n",
    "1. *Upstream dependencies*: These are the tasks that must be executed before a given task. In other words, a task's upstream dependencies are the tasks that produce the data or the resources required for the task to run.\n",
    "2. *Downstream dependencies*: These are the tasks that depend on a given task. In other words, a task's downstream dependencies are the tasks that require the output of the task to run.\n",
    "\n",
    "<br>**<u>Task configuration</u>**\n",
    "\n",
    "Task configuration refers to the various settings and options that can be specified when defining a task within a DAG. These settings determine the behavior of the task and can include things such as priority, retries, and how to handle errors. The configuration of a task is defined using Python code when creating the task instance. Each operator in Airflow has its own set of configuration options, which can be specified as keyword arguments when the operator is instantiated. Some examples of common configuration options that can be set for a task include:\n",
    "\n",
    "- `priority_weight`: This option allows you to set the priority of the task within the DAG. Tasks with higher priority weights will be executed first.\n",
    "- `retries`: This option determines how many times the task will be retried if it fails. By default, Airflow will attempt to retry the task once, but this can be configured to a higher number if needed.\n",
    "- `email_on_failure`: This option determines whether Airflow should send an email notification when the task fails.\n",
    "\n",
    "By configuring these options, you can customize the behavior of each task to meet your specific requirements and ensure that your workflows run smoothly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DAG\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cron Expressions\n",
    "\n",
    "A cron expression is a string that specifies a set of times when a task should be executed. Airflow uses cron expressions to schedule tasks within a DAG. When you create a DAG in Airflow, you specify the schedule interval using a cron expression. Airflow then uses this expression to determine when to trigger each task within the DAG. A cron expression is made up of five fields, separated by spaces, that specify when the task should be executed. The fields are:\n",
    "\n",
    "```\n",
    "*    *    *    *    *\n",
    "┬    ┬    ┬    ┬    ┬\n",
    "│    │    │    │    │\n",
    "│    │    │    │    │\n",
    "│    │    │    │    └───── day of the week (0 - 6, Sunday to Saturday)\n",
    "│    │    │    └────────── month (1 - 12)\n",
    "│    │    └─────────────── day of the month (1 - 31)\n",
    "│    └──────────────────── hour (0 - 23)\n",
    "└───────────────────────── minute (0 - 59)\n",
    "```\n",
    "\n",
    "Each field in a cron expression can contain one of the following:\n",
    "\n",
    "A wildcard (*), which means all possible values for that field.\n",
    "A range of values (e.g. 5-10), which means any value within that range.\n",
    "A list of values (e.g. 1,3,5), which means any value within that list.\n",
    "A step value (e.g. */5), which means every 5th value, starting from the minimum value.\n",
    "A combination of these (e.g. */5 9-17 * * *), which means every 5 minutes between 9:00am and 5:59pm, every day.\n",
    "\n",
    "For example, if you want to schedule a task to run every day at 3am, you can set the cron expression to `0 3 * * *`, which means \"at minute 0, hour 3, every day, every month, every day of the week\".\n",
    "\n",
    "<br>**Getting help:**\n",
    "\n",
    "There are several websites that can help you generate or read cron expressions. One that allows you to enter a cron expression and see the schedule in human-readable form is <a href=\"URL\">crontab.guru</a>. You can also enter a schedule in plain English and get a corresponding cron expression.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
