{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ETL: Load</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is transformed, it is loaded into the destination system, such as a data warehouse or a database. The data engineer needs to design and implement a data loading process that is efficient and reliable. This may involve creating tables and indexes, optimizing queries, and monitoring the data loading process to ensure that it completes successfully."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLAP and OLTP Databases\n",
    "\n",
    "Loading data into a database will depend on how they are going to be processed. OLAP (Online Analytical Processing) and OLTP (Online Transaction Processing) are two distinct approaches to processing data that are often used with analytical and transactional databases, respectively.\n",
    "\n",
    "Analytical databases are designed for complex analytical queries that involve aggregating and analyzing large volumes of data to gain insights and make data-driven decisions. These queries may involve complex joins, filtering, and aggregation operations that require a lot of processing power and time to complete. Analytical databases are optimized for fast reads and complex queries, often using specialized indexing and data storage techniques to speed up query execution times. \n",
    "\n",
    "In contrast, transactional databases are designed for fast, reliable transaction processing. These databases are optimized for small, discrete operations such as creating, updating, or deleting records from a database. Transactions need to be processed quickly and with high reliability to ensure that the database remains consistent and that data is not lost or corrupted. Transactional databases are optimized for fast read/write operations and transactional consistency, often using techniques such as locking and rollback to ensure data integrity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-oriented and Row-oriented Databases\n",
    "\n",
    "Row-oriented and column-oriented databases are both structured in the same way, with tables consisting of rows and columns. However, they operate differently in terms of how they store, retrieve, and process data. \n",
    "\n",
    "Performing analytics on row-oriented databases is costly since querying for specific columns involves retrieving the entire row and only then extracting the specified columns. In contrast, column-oriented databases work by retrieving only the relevant columns and are therefore preferred for analytics.\n",
    "\n",
    "Row-oriented databases are preferred for transactional processing because they are optimized for fast read and write operations. In a transactional processing system, it is important to be able to quickly retrieve and update individual records, often in response to user input or other real-time events. Row-oriented databases excel at these types of operations because they store data in a row-by-row format, with all the fields for a single record stored together in a single row. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massively Parallel Processing (MPP) Databases\n",
    "\n",
    "Column-oriented databases are better suited for OLAP operations and due to the way they store data, they are better for parallelization. MPP databases are column-oriented databases that run in a distributed fashion. Specifically, queries are not execute on a single node, but rather split into subtasks and distributed among several nodes.\n",
    "\n",
    "MPP databases can be a target of an ETL process, especially in the context of data warehousing and analytics. In this scenario, data is extracted from various sources, transformed into a format that is suitable for analysis, and then loaded into the MPP database for storage and querying.\n",
    "\n",
    "Examples of MPP databases include:\n",
    "-\tAmazon Redshift\n",
    "-\tAzure SQL Data Warehouse\n",
    "-\tGoogle BigQuery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data into Amazon Redshift\n",
    "\n",
    "Steps for loading data into Amazon Redshift:\n",
    "1. Write files to appropriate storage format <br>\n",
    "Loading data to an MPP database from csv files would not be a good option. These databases typically load data from files that use a column-oriented format such as Parquet. Parquet is a columnar storage file format that is commonly used for storing and processing large datasets in a distributed computing environment. \n",
    "\n",
    "2. Send a copy query to Redshift <br>\n",
    "A copy query in Amazon Redshift is used to load data from an external data source into a Redshift table. The data can be in various formats such as CSV, JSON, or Parquet, and can be located in Amazon S3, Amazon EMR, or other remote hosts. To send a copy query in Amazon Redshift, you can use any SQL client tool that supports Redshift, such as SQL Workbench/J, pgAdmin, or the AWS Management Console. You can also use the AWS Command Line Interface (CLI) or one of the Redshift APIs, such as JDBC or ODBC, to submit the copy query."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Writing a Pandas and a PySpark DataFrame to parquet in S3 (Amazon Simple Storage Service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas .to_parquet() method \n",
    "df.to_parquet(\"s3://path/to/bucket/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark .write.parquet() method\n",
    "df.write.parquet(\"s3://path/to/bucket/data.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Executing the copy query using sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Set up the connection to Redshift\n",
    "engine = create_engine('postgresql+psycopg2://your-redshift-user:your-redshift-password@your-redshift-host:port/your-redshift-db')\n",
    "\n",
    "# Execute the COPY command\n",
    "copy_query = \"\"\"\n",
    "COPY data\n",
    "FROM 's3://path/to/bucket/data.parquet'\n",
    "FORMAT AS parquet;\n",
    "\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(copy_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Loading data into PostgreSQL\n",
    "\n",
    "To load data from a Pandas DataFrame into a PostgreSQL database, you can use the to_sql() method of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# create a database connection using SQLAlchemy\n",
    "engine = create_engine('postgresql://user:password@host:port/database')\n",
    "\n",
    "# load the dataframe into the database\n",
    "df.to_sql('table_name', \n",
    "          engine, \n",
    "          if_exists='replace', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The if_exists parameter specifies what to do if the table already exists in the database. If set to ```'replace'```, it will replace the table with the dataframe. If set to ```'append'```, it will append the dataframe to the existing table. If set to ```'fail'```, it will raise an error if the table already exists. The index parameter specifies whether or not to include the dataframe's index as a column in the database table. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark: Loading data into PostgreSQL\n",
    "\n",
    "Loading a PySpark DataFrame into a PostgreSQL database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create a database connection using SQLAlchemy\n",
    "url = 'postgresql://user:password@host:port/database'\n",
    "table_name = 'table_name'\n",
    "mode = 'overwrite'  # Options: 'append', 'overwrite', 'error'\n",
    "\n",
    "# write the PySpark DataFrame to the database\n",
    "df.write.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", table_name).option(\"mode\", mode).save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode parameter specifies what to do if the table already exists in the database. If set to 'overwrite', it will replace the table with the PySpark DataFrame. If set to 'append', it will append the PySpark DataFrame to the existing table. If set to 'error', it will raise an error if the table already exists.\n",
    "\n",
    "The last line of code writes the PySpark DataFrame to the PostgreSQL database using the JDBC API. Here's a breakdown of what's happening:\n",
    "\n",
    "- ```df``` is the PySpark DataFrame that we want to write to the database.\n",
    "- ```write``` is a method that starts the process of writing the DataFrame to an external storage system.\n",
    "- ```format(\"jdbc\")``` tells PySpark that we want to use the JDBC API to write to the database.\n",
    "- ```option(\"url\", url)``` specifies the URL of the database we want to connect to.\n",
    "- ```option(\"dbtable\", table_name)``` specifies the name of the table we want to create in the database.\n",
    "- ```option(\"mode\", mode)``` specifies the write mode, which can be 'overwrite', 'append', or 'error'.\n",
    "- ```save()``` is the method that executes the write operation and saves the DataFrame to the database.\n",
    "\n",
    "Note: you will need to have the PostgreSQL JDBC driver installed in order to use this. You can download the driver from the PostgreSQL website, and then add it to your PySpark application's classpath by including the path to the JAR file in the spark.driver.extraClassPath configuration property."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
